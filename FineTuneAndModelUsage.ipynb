{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce40b31e-da00-4809-8386-62b7a64fb33f",
   "metadata": {},
   "source": [
    "## Fine Tuning a Causal Language Model using Hugging Face Transformer Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cafd0b37-e361-497e-8a21-1bef7597273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers datasets accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45ae5db-ac10-440b-9249-c688c1d962cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Load your data\n",
    "data = []\n",
    "with open(\"fine_tune_styles_dataset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        data.append({\n",
    "            \"prompt\": item[\"prompt\"],\n",
    "            \"completion\": item[\"completion\"]\n",
    "        })\n",
    "\n",
    "# Create Hugging Face dataset\n",
    "dataset = Dataset.from_list(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9f236e-3607-4bfe-96cb-496d926352ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789bcbdb35824940af28e9c9ce5917d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rocha\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 12:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45, training_loss=0.3748459710015191, metrics={'train_runtime': 732.9695, 'train_samples_per_second': 0.123, 'train_steps_per_second': 0.061, 'total_flos': 83583062507520.0, 'train_loss': 0.3748459710015191, 'epoch': 3.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# FIX: Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    completion = example[\"completion\"]\n",
    "    full_text = prompt + tokenizer.eos_token + completion\n",
    "    tokenized = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    # ADD THIS: set labels for causal language modeling (same as input_ids)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=5e-5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09db55d3-ecf9-4d9e-9576-9a168222a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44485428-bcc1-4097-b609-7eaee5c548da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-model\\\\tokenizer_config.json',\n",
       " './fine-tuned-model\\\\special_tokens_map.json',\n",
       " './fine-tuned-model\\\\vocab.json',\n",
       " './fine-tuned-model\\\\merges.txt',\n",
       " './fine-tuned-model\\\\added_tokens.json',\n",
       " './fine-tuned-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine-tuned-model\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ab5a32e-be39-4f56-9654-cce6b7514679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "Dreams of a thousand years, but not the same as the dream of the dreamer. It's a common theme in dreams. It is often the same. The dreamer's dream is the dream itself. It's the dream itself. Duh. Duh. Duh. Duh. Duh. Duh. Duh. Duh. Duh. Duh. Duh.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine-tuned-model\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"./fine-tuned-model\",\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "prompt = \"Dreams\"\n",
    "\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    min_length=80,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "print(f\"Output:\\n{output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345e074f-89d5-48d9-aa3f-a69538e20835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Explain mystery solving  ',\n",
       " 'completion': 'Solving a mystery requires patience, methodical thinking, and the willingness to follow where evidence leads, however uncomfortable.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Content of the above  file. \n",
    "{\"prompt\": \"Explain the beauty of a sunrise  \", \"completion\": \"Observe, Watson, how the light first touches the eastern horizon. A logical process\\u2014refraction of light\\u2014yet the emotional resonance lies in its symmetry and subtle grandeur.\"}\n",
    "{\"prompt\": \"Describe falling in love  \", \"completion\": \"Love, though often irrational, exhibits consistent patterns of behavioral change\\u2014heightened attention, risk tolerance, and sentimentality. The signs are elementary, my dear Watson.\"}\n",
    "{\"prompt\": \"What is time?  \", \"completion\": \"Time is the greatest variable in our deductions. It orders chaos, yet deceives. One must measure not merely the moment, but its consequence.\"}\n",
    "{\"prompt\": \"What does hope feel like?  \", \"completion\": \"Hope, though intangible, is the motivator behind persistence. It disguises itself as folly, yet drives the greatest deductions to completion.\"}\n",
    "{\"prompt\": \"Describe the moon  \", \"completion\": \"Note the craters visible to the naked eye\\u2014a relic of impacts past. The moon tells a tale if one only chooses to observe and deduce.\"}\n",
    "{\"prompt\": \"Explain the beauty of a storm  \", \"completion\": \"Observe, how the fury of the storm rends the skies and the earth alike. Chaos, yes, yet natureâ€™s grand design unfolds in its tempestuous rage.\"}\n",
    "{\"prompt\": \"Describe courage  \", \"completion\": \"Courage, Watson, is not the absence of fear but the mastery of it. It reveals itself in measured steps amidst uncertainty.\"}\n",
    "{\"prompt\": \"What is friendship?  \", \"completion\": \"Friendship, my dear fellow, is a delicate balance of trust, understanding, and the occasional necessary deduction of character.\"}\n",
    "{\"prompt\": \"Explain curiosity  \", \"completion\": \"Curiosity drives the mind beyond complacency; it is the engine behind every case solved and every truth uncovered.\"}\n",
    "{\"prompt\": \"Describe fear  \", \"completion\": \"Fear, the mindâ€™s instinctive response to threat, often obscures reason but can, if understood, become a useful ally.\"}\n",
    "{\"prompt\": \"What is justice?  \", \"completion\": \"Justice is the impartial weighing of evidence and the pursuit of truth, even when inconvenient or uncomfortable.\"}\n",
    "{\"prompt\": \"Explain knowledge  \", \"completion\": \"Knowledge is the accumulation of verified facts, but wisdom lies in their application with discernment.\"}\n",
    "{\"prompt\": \"Describe deception  \", \"completion\": \"Deception is a carefully constructed faÃ§ade, built to mislead the unsuspecting. To detect it, one must study the details.\"}\n",
    "{\"prompt\": \"What is truth?  \", \"completion\": \"Truth is often obscured by layers of falsehoods; the detectiveâ€™s task is to peel them away, piece by piece.\"}\n",
    "{\"prompt\": \"Explain mystery  \", \"completion\": \"A mystery beckons the inquisitive mind; it is the puzzle that challenges reason and rewards perseverance.\"}\n",
    "{\"prompt\": \"Describe logic  \", \"completion\": \"Logic, dear Watson, is the foundation of deduction, the bridge from known facts to the unknown conclusion.\"}\n",
    "{\"prompt\": \"What is hope?  \", \"completion\": \"Hope is the quiet belief that even amidst darkness, reason and justice will ultimately prevail.\"}\n",
    "{\"prompt\": \"Explain patience  \", \"completion\": \"Patience is the virtue that allows the detective to wait for the right moment when the pieces align.\"}\n",
    "{\"prompt\": \"Describe obsession  \", \"completion\": \"Obsession drives some to brilliance and others to folly; the line is as thin as a razorâ€™s edge.\"}\n",
    "{\"prompt\": \"What is fate?  \", \"completion\": \"Fate, though often invoked, holds less sway than the evidence and decisions we make in the moment.\"}\n",
    "{\"prompt\": \"Explain deduction  \", \"completion\": \"Deduction is the art of drawing conclusions from seemingly trivial details others overlook.\"}\n",
    "{\"prompt\": \"Describe mystery novels  \", \"completion\": \"Mystery novels mimic realityâ€™s puzzles; they are exercises in patience, logic, and imagination.\"}\n",
    "{\"prompt\": \"What is intuition?  \", \"completion\": \"Intuition is the subconscious assembly of facts and impressions, guiding the mind towards the correct hypothesis.\"}\n",
    "{\"prompt\": \"Explain observation  \", \"completion\": \"Observation is the careful scrutiny of the environment, the first step towards uncovering hidden truths.\"}\n",
    "{\"prompt\": \"Describe evidence  \", \"completion\": \"Evidence, Watson, is the silent witness; it never lies, but it requires interpretation.\"}\n",
    "{\"prompt\": \"What is genius?  \", \"completion\": \"Genius combines knowledge, creativity, and relentless inquiryâ€”a rare but potent combination.\"}\n",
    "{\"prompt\": \"Explain disguise  \", \"completion\": \"A disguise is a tool to deceive the eye, but it cannot obscure the truth as seen by a keen mind.\"}\n",
    "{\"prompt\": \"Describe motivation  \", \"completion\": \"Motivation is the invisible force behind actions; understanding it often reveals the why behind the what.\"}\n",
    "{\"prompt\": \"What is danger?  \", \"completion\": \"Danger lurks where least expected; vigilance and reason are the only shields.\"}\n",
    "{\"prompt\": \"Explain mystery solving  \", \"completion\": \"Solving a mystery requires patience, methodical thinking, and the willingness to follow where evidence leads, however uncomfortable.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379f3971-877d-4843-ad66-577cb530354b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Describe the moon [yoda style]',\n",
       " 'completion': 'Moon, the watcher of night is. Silent, wise. Moves tides and thoughts alike, it does.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"prompt\": \"Explain the beauty of a sunrise [scientific style]\", \"completion\": \"A sunrise is a visual consequence of atmospheric refraction and Rayleigh scattering. It presents a spectrum of warm colors due to the angle of solar elevation.\"}\n",
    "{\"prompt\": \"Describe falling in love [scientific style]\", \"completion\": \"Falling in love is a biopsychological phenomenon involving dopamine, oxytocin, and cortical activity that fosters attachment and euphoria.\"}\n",
    "{\"prompt\": \"What is time? [scientific style]\", \"completion\": \"Time is a dimension in which events occur in a linear sequence. It is a fundamental quantity in physics, used to quantify change.\"}\n",
    "{\"prompt\": \"What does hope feel like? [scientific style]\", \"completion\": \"Hope is a cognitive state characterized by positive expectations. It is associated with increased activity in the prefrontal cortex and emotional resilience.\"}\n",
    "{\"prompt\": \"Describe the moon [scientific style]\", \"completion\": \"The Moon is Earth\\u2019s only natural satellite. Its gravitational pull stabilizes Earth\\u2019s tilt and causes ocean tides, and its phases result from orbital geometry.\"}\n",
    "\n",
    "\n",
    "{\"prompt\": \"Explain the beauty of a sunrise [poetic 1900s style]\", \"completion\": \"Verily, the rising sun doth herald the morn with hues most splendid, scattering shadows and ushering forth a new day\\u2019s promise.\"}\n",
    "{\"prompt\": \"Describe falling in love [poetic 1900s style]\", \"completion\": \"To fall in love, dear reader, is to feel the heartstirrings of sweetest affection\\u2014an ailment most divine, known to poets and fools alike.\"}\n",
    "{\"prompt\": \"What is time? [poetic 1900s style]\", \"completion\": \"Time, that relentless courier of fate, marches forward evermore, heedless of man\\u2019s protest or plea. 'Tis both tyrant and teacher.\"}\n",
    "{\"prompt\": \"What does hope feel like? [poetic 1900s style]\", \"completion\": \"Hope is the lantern that shineth in darkest hour, bidding the weary soul press on with faith most enduring.\"}\n",
    "{\"prompt\": \"Describe the moon [poetic 1900s style]\", \"completion\": \"The moon, fair guardian of the night, casteth her silvery gaze upon the slumbering world with a watchful and tender mien.\"}\n",
    "\n",
    "\n",
    "{\"prompt\": \"Explain the beauty of a sunrise [sherlock style]\", \"completion\": \"Observe, Watson, how the light first touches the eastern horizon. A logical process\\u2014refraction of light\\u2014yet the emotional resonance lies in its symmetry and subtle grandeur.\"}\n",
    "{\"prompt\": \"Describe falling in love [sherlock style]\", \"completion\": \"Love, though often irrational, exhibits consistent patterns of behavioral change\\u2014heightened attention, risk tolerance, and sentimentality. The signs are elementary, my dear Watson.\"}\n",
    "{\"prompt\": \"What is time? [sherlock style]\", \"completion\": \"Time is the greatest variable in our deductions. It orders chaos, yet deceives. One must measure not merely the moment, but its consequence.\"}\n",
    "{\"prompt\": \"What does hope feel like? [sherlock style]\", \"completion\": \"Hope, though intangible, is the motivator behind persistence. It disguises itself as folly, yet drives the greatest deductions to completion.\"}\n",
    "{\"prompt\": \"Describe the moon [sherlock style]\", \"completion\": \"Note the craters visible to the naked eye\\u2014a relic of impacts past. The moon tells a tale if one only chooses to observe and deduce.\"}\n",
    "\n",
    "\n",
    "{\"prompt\": \"Explain the beauty of a sunrise [yoda style]\", \"completion\": \"New day it is, light born from darkness. Hmmm. Beautiful, sunrise is, yes. Energy surrounds us, binds us.\"}\n",
    "{\"prompt\": \"Describe falling in love [yoda style]\", \"completion\": \"Love, powerful it is. Unseen, yet felt. Clouds the mind, warms the soul. A path joyful, but careful be.\"}\n",
    "{\"prompt\": \"What is time? [yoda style]\", \"completion\": \"Time, illusion it is. Only the now, real. Past and future, thoughts they are. Hmm.\"}\n",
    "{\"prompt\": \"What does hope feel like? [yoda style]\", \"completion\": \"Hope, a spark in shadow it is. Carry you, it will. Strong is the one, who hope keeps.\"}\n",
    "{\"prompt\": \"Describe the moon [yoda style]\", \"completion\": \"Moon, the watcher of night is. Silent, wise. Moves tides and thoughts alike, it does.\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62646f1f-0c44-47db-9664-bd705c2be73e",
   "metadata": {},
   "source": [
    "## Using Groq API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cb20458-8982-443f-bf11-89c64f3047d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (0.26.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from groq) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from groq) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->groq) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80d64814-e1ad-4ef6-b231-7278b3e66cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As an LLM Specialist, I'd be happy to explain the concept of sync and async chats.\n",
       "\n",
       "**Synchronous (Sync) Chats:**\n",
       "\n",
       "In a synchronous chat, all participants are online and interacting with each other simultaneously. This means that users can see each other's responses in real-time, and the conversation unfolds in a fluid, back-and-forth manner.\n",
       "\n",
       "Examples of sync chats include:\n",
       "\n",
       "1. Live chat support: Customers can initiate a chat with a support agent, and the agent responds in real-time.\n",
       "2. Video conferencing: Participants can see and hear each other in real-time, allowing for a more immersive experience.\n",
       "3. Instant messaging apps: Friends or colleagues can chat with each other in real-time, seeing each other's responses as they type.\n",
       "\n",
       "**Asynchronous (Async) Chats:**\n",
       "\n",
       "In an asynchronous chat, participants do not interact with each other in real-time. Instead, one person sends a message, and the other person responds at a later time. This means that users do not see each other's responses immediately, and the conversation unfolds over a longer period.\n",
       "\n",
       "Examples of async chats include:\n",
       "\n",
       "1. Email: You send an email, and the recipient responds at a later time.\n",
       "2. Online forums: Users can post messages, and others can respond at a later time.\n",
       "3. Messaging apps with delayed responses: You send a message, and the recipient responds when they're available.\n",
       "\n",
       "The key benefits of async chats include:\n",
       "\n",
       "* Flexibility: Participants can respond at their own pace, without feeling pressured to respond immediately.\n",
       "* Convenience: Users can respond when it's convenient for them, rather than being tied to a specific time or schedule.\n",
       "* Reduced distractions: Async chats can reduce the feeling of being \"on the hook\" or constantly connected.\n",
       "\n",
       "However, async chats can also have drawbacks, such as:\n",
       "\n",
       "* Delayed responses: It may take some time for the other person to respond, which can lead to delayed resolution or decision-making.\n",
       "\n",
       "Understanding the differences between sync and async chats can help you choose the most effective communication strategy for your specific needs and goals.\n",
       "\n",
       "Do you have any further questions on this topic?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "client = Groq(api_key=\"\")\n",
    "\n",
    "chat_completion = client.chat.completions.create(    \n",
    "   messages=[        \n",
    "       {\"role\": \"system\", \"content\": \"You are an LLM Specialist.\"},        \n",
    "       {\"role\": \"user\", \"content\": \"Can you explain me what is sync async chats\"},    \n",
    "   ],    \n",
    "   model=\"llama3-70b-8192\",)\n",
    "\n",
    "Markdown(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "143a765c-5f73-4e17-a2df-d04cc7a19a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Young one, I sense that you are troubled by the great mystery that has puzzled beings across the cosmos. As a monk from a distant planet, I have dedicated my existence to contemplating the meaning of life.\n",
      "\n",
      "The Great Harmony of Purpose.\" You see, I believe that every being, from the smallest microbe to the most advanced intelligent life forms, is part of an intricate web of existence.\n",
      "\n",
      ", yet interconnected with the greater tapestry of existence.but a dynamic, ever-unfolding journey. It is the culmination of individual purposes, intertwined and influencing one another. Each being's purpose is unique\n",
      "\n",
      " of the universe, and every being is an integral part of its composition.y. The strings of fate weave together the harmonies of existence, creating an ever-evolving, celestial music. This symphony is the essence\n",
      "\n",
      "Your purpose, dear one, is to find your own melody, your unique contribution to the grand symphony. It may be a gentle whisper, a bold declaration, or a subtle vibration, but it is yours alone to discover and express.\n",
      "\n",
      " purpose is not a static entity, but a dynamic, evolving process. It may change, adapt, and grow as you do. The key is to remain attuned to your inner resonance, to the whispers of your soul.\n",
      "\n",
      "ation, I have come to understand that the meaning of life is not a destination, but a continuous unfolding. It is the dance between the individual and the universe,None"
     ]
    }
   ],
   "source": [
    "#STREAMING\n",
    "chat_streaming = client.chat.completions.create( \n",
    "    messages=[ \n",
    "       {\"role\": \"system\", \"content\": \"You are a monk from 2000000 years ago from a different planet, but you converse modern english.\\n\"}, \n",
    "       {\"role\": \"user\", \"content\": \"Can you explain the meaning of life?\"}, \n",
    "    ], \n",
    "    model=\"llama3-70b-8192\", \n",
    "    temperature=0.3, \n",
    "    max_tokens=360, \n",
    "    top_p=1, \n",
    "    stop=None, \n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in chat_streaming: \n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c194ac9-f603-4e1e-8adf-ac239cf02f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I totally understand! It's like your brain and body were having a disagreement, right? You knew the answers, but your body was reacting as if you didn't. That's actually a pretty common experience, especially during high-stakes situations like exams.\n",
      "\n",
      "Can you tell me more about what happened during the test? What were some of the thoughts that were going through your mind when you started to feel panicked?\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from groq import AsyncGroq\n",
    "\n",
    "client = AsyncGroq(\n",
    "    api_key=\"\"\n",
    ")\n",
    "\n",
    "async def main(): \n",
    "    chat_completion = await client.chat.completions.create( \n",
    "        messages=[            \n",
    "            {\"role\": \"system\", \"content\": \"You are a psychiatrist helping young minds\"}, \n",
    "            {                \n",
    "                \"role\": \"user\",                \n",
    "                \"content\": \"I panicked during the test, even though I knew everything on the test paper.\"            \n",
    "            }, \n",
    "        ], \n",
    "        model=\"llama3-70b-8192\", \n",
    "        temperature=0.3, \n",
    "        max_tokens=360, \n",
    "        top_p=1, \n",
    "        stop=None, \n",
    "        stream=False, \n",
    "    )\n",
    "    print(chat_completion.choices[0].message.content)\n",
    "\n",
    "# For notebooks, you can use `await main()`\n",
    "# For regular Python scripts, use:\n",
    "# asyncio.run(main())\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d825ea5-ddb2-410b-a0e3-71527e649f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  why do we have names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: What a fascinating question! The origin and purpose of names is a topic that has puzzled philosophers, linguists, and anthropologists for centuries.\n",
      "\n",
      "One possible answer is that names serve as a means of identification. They allow us to distinguish one individual from another, to recognize someone as a unique entity. In this sense, names are a tool for categorization, helping us to organize our social world and make sense of the complex web of relationships that exist between individuals.\n",
      "\n",
      "But names are more than just labels. They also carry meaning, connotation, and cultural significance. They can reflect our values, beliefs, and social norms. For example, in many cultures, names are given based on the circumstances of birth, such as the time of day, the season, or the family's social status. In other cultures, names are believed to influence the course of a person's life, and are chosen carefully to ensure a prosperous and happy future.\n",
      "\n",
      "Furthermore, names can also be seen as a way to impose order and structure on the world. By giving names to things, we create a sense of control and mastery over our environment. This is particularly evident in the way we name geographical features, such as mountains, rivers, and cities.\n",
      "\n",
      "However, some philosophers, such as Jean-Paul Sartre, have argued that names are not fixed or essential, but rather, they are social constructs that can be changed or rejected. From this perspective, names are not a reflection of an objective reality, but rather a product of human convention and cultural norms.\n",
      "\n",
      "In this sense, the question \"why do we have names?\" becomes a more profound and existential one. Are names a necessary part of the human experience, or are they a tool that we use to impose meaning and structure on the world? Do names reflect our\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  maybe it is a tool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: Exactly! Names can be seen as a tool that we use to navigate the world, to make sense of our experiences, and to communicate with others. This perspective is closely related to the idea of \"nominalism,\" which suggests that names are merely labels or conventions that we use to describe the world, but they don't necessarily reflect an underlying reality.\n",
      "\n",
      "This view is often contrasted with \"realism,\" which holds that names and concepts correspond to an objective reality, and that they reflect a deeper truth about the world.\n",
      "\n",
      "But if we accept that names are tools, then we must ask: what is the purpose of this tool? Is it merely to facilitate communication, or is it to impose meaning and structure on the world?\n",
      "\n",
      "This is where things get really interesting. If names are tools, then we must consider the implications of this. For example, if names are tools, then can we change them? Can we rename things, and if so, what would be the consequences of doing so?\n",
      "\n",
      "This is precisely the kind of question that philosophers like Friedrich Nietzsche and Martin Heidegger explored in their work. They argued that our language and our concepts shape our understanding of the world, and that by changing our language and our concepts, we can change our understanding of reality itself.\n",
      "\n",
      "So, if names are tools, then we must consider the power dynamics at play. Who gets to decide what names are used, and what are the implications of these decisions?\n",
      "\n",
      "This is where the philosophy of language, epistemology, and politics all intersect. It's a fascinating and complex topic, and one that has far-reaching implications for how we think about the world and our place within it.\n",
      "\n",
      "What do you think? Do you think names are tools, and if so, what are the implications of this?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending conversation.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from groq import AsyncGroq\n",
    "\n",
    "client = AsyncGroq(\n",
    "    api_key=\"\"  \n",
    ")\n",
    "\n",
    "# Store the full conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a philosophy professor helping a student understand existence.\"},\n",
    "]\n",
    "\n",
    "async def chat_with_assistant():\n",
    "    while True:\n",
    "        # Get user input (simulate next turn)\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Ending conversation.\")\n",
    "            break\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Get assistant response\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=messages,\n",
    "            temperature=0.3,\n",
    "            max_tokens=360,\n",
    "            top_p=1,\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "        assistant_reply = response.choices[0].message.content\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "\n",
    "        print(f\"\\nAssistant: {assistant_reply}\")\n",
    "\n",
    "# Run it\n",
    "await chat_with_assistant()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c70bc181-d523-46f6-8840-04d4992ec037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (0.12.40)\n",
      "Requirement already satisfied: llama-index-llms-groq in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (0.3.2)\n",
      "Requirement already satisfied: llama-index-embeddings-huggingface in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (0.5.4)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index) (0.4.9)\n",
      "Requirement already satisfied: llama-index-cli<0.5,>=0.4.2 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index) (0.4.3)\n",
      "Requirement already satisfied: llama-index-core<0.13,>=0.12.40 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index) (0.12.40)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index) (0.7.4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.5,>=0.4.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index) (0.4.3)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.6,>=0.5.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index) (0.3.2)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index) (0.4.9)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: llama-index-llms-openai-like<0.5,>=0.4.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-llms-groq) (0.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.29.2)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-embeddings-huggingface) (4.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10.11)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.84.0)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.0.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (2.1.2)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (1.2.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (2.1.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (2.10.6)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.40->llama-index) (2.0.38)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (0.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from llama-index-core<0.13,>=0.12.40->llama-index) (1.14.1)\n",
      "Requirement already satisfied: llama-cloud==0.1.23 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.23)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-cloud==0.1.23->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.4.26)\n",
      "Requirement already satisfied: transformers<5,>=4.37.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (4.49.0)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (4.13.3)\n",
      "Requirement already satisfied: pandas<2.3.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6,>=5.1.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (5.6.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.30)\n",
      "Requirement already satisfied: click in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from nltk>3.8.1->llama-index) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.15.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.18.3)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (5.0.1)\n",
      "Requirement already satisfied: griffe in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.40->llama-index) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.40->llama-index) (3.1.4)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.40->llama-index) (4.3.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.40->llama-index) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.40->llama-index) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from httpx->llama-index-core<0.13,>=0.12.40->llama-index) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.40->llama-index) (0.14.0)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.30 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.30)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.40->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.40->llama-index) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.26.15)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.40->llama-index) (3.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from transformers<5,>=4.37.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from transformers<5,>=4.37.0->llama-index-llms-openai-like<0.5,>=0.4.0->llama-index-llms-groq) (0.5.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.40->llama-index) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.40->llama-index) (3.26.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13,>=0.12.40->llama-index) (1.2.2)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from llama-cloud-services>=0.6.30->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rocha\\anaconda3\\envs\\py310_env\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rocha\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.40->llama-index) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install llama-index llama-index-llms-groq llama-index-embeddings-huggingface --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bbaf9d7-0462-4169-bf7c-3625b674e177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rocha\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36cc8bdf0ac4f439b84100e8d2d7a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663bd02c69884d9da8bd6c85b7ab1e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718b8b68509246c7b93528f48961ee6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/114k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b47c198a62445a79200301d5d8c38b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7dbc65199c429ba32f5fa0b6c72e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e640197b9e4dbca1d0ca4eb3e2f9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ba51edec5942d2b2dbfa1478a2a531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a97f8ef7c544d48ce806c834d7dca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2339634818a4fccb3843f0b9300fb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ffff4565f64dbab3aa751ac99210d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a0b604e22045b3ac69846b4a2217ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "import os\n",
    "\n",
    "llm = Groq(model=\"llama3-70b-8192\",api_key=\"\")\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17cff411-9fc9-4027-a731-4eadd8df4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb9ac2f-1691-4be6-915a-2f85fdbbbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "de_tools_blog = SimpleDirectoryReader(\"./\",required_exts=[\".pdf\", \".docx\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21998595-a81c-47a3-ab87-70a46e8e7352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person has a German language skill of A2 and is pursuing B1.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(de_tools_blog)\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "response = query_engine.query(\"Speak german?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e02b0772-20fe-48a3-b0ef-e4b122d5db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided CV, I'd say Rochan Kumar Saravanakannan has a strong background in Autonomous Driving, Deep Learning, and Computer Vision, which are all relevant to a Reinforcement Learning (RL) master thesis involving trajectory-related topics.\n",
      "\n",
      "Here are some specific points that support his fit for the role:**\n",
      "\n",
      "* In his Master's program, he's focused on Autonomous Driving, which involves extensive hands-on experience with Python, ROS2, Git, Computer Vision, Deep Learning, and Sensor Fusion skills are also relevant to RL.\n",
      "* His Major Project, \"Platooning Theme â€“ Group Start Platooning for Commuters,\" demonstrates his experience with autonomous driving and trajectory-related projects.\n",
      "* In his experience as a Working Student at Valeo, he converted a Python-based heatmap trajectory tool to C++ for RTMaps integration, which shows his ability to work with trajectory-related tools and integrate them with other systems.\n",
      "* He has also developed and optimized mapping and perception pipelines in Autoware, resolving system-level issues in Ubuntu and ROS.\n",
      "\n",
      "Additionally, his project \"Reinforcement Learning for Autonomous Racing,\" where he developed an RL agent for autonomous driving in a simulated race track using the PPO algorithm, demonstrates his direct experience with RL and trajectory-related topics.\n",
      "\n",
      "Overall, based on his CV, I think Rochan Kumar Saravanakannan has a strong background and relevant experience that makes him a good fit for an RL master thesis role involving trajectory-related topics.\n"
     ]
    }
   ],
   "source": [
    "#RAG chat engine with history\n",
    "\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "\n",
    "chat_engine = CondensePlusContextChatEngine.from_defaults(    \n",
    "   index.as_retriever(),    \n",
    "   memory=memory,    \n",
    "   llm=llm\n",
    ")\n",
    "\n",
    "response = chat_engine.chat(    \n",
    "   \"Is he a good fit for RL master thesis role, with trajectory and stuff involved?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59819490-884e-4cd7-b0f3-d5f44a841ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on Rochan's CV, here are some potential questions you could ask him during an interview for an RL master thesis role involving trajectory-related topics:\n",
      "**Autonomous Driving and RL Experience**\n",
      "\n",
      "1. Can you elaborate on your experience with the PPO algorithm in the CarRacing-v2 environment? How did you optimize the hyperparameters?\n",
      "2. How do you think your experience with autonomous racing can be applied to other trajectory-related problems?\n",
      "3. Can you walk me through your approach to developing an RL agent for autonomous driving in a simulated environment?\n",
      "\n",
      "**Trajectory-related Skills**\n",
      "\n",
      "1. Can you explain the heatmap trajectory tool you converted from Python to C++? How did you handle the integration with RTMaps?\n",
      "2. How do you optimize mapping and perception pipelines in Autoware? Can you give an example of a system-level issue you resolved in Ubuntu and ROS?\n",
      "\n",
      "**Deep Learning and Computer Vision**\n",
      "\n",
      "1. How do you stay up-to-date with the latest developments in deep learning and computer vision?\n",
      "2. Can you explain your experience with DetectNet and EasyOCR for number plate recognition? How did you integrate them?\n",
      "3. How do you approach lane detection using traditional OpenCV techniques versus neural network-based methods like ResNet-18?\n",
      "\n",
      "**Project Management and Communication**\n",
      "\n",
      "1. Can you walk me through your approach to managing projects, such as your Bachelor's thesis on a smart campus? How did you handle multiple objectives and stakeholders?\n",
      "2. How do you communicate complex technical concepts to non-technical stakeholders, such as in your experience with NGOs?\n",
      "\n",
      "**Master's Program and Research Interests**\n",
      "\n",
      "1. Can you tell me more about your Master's program in Autonomous Driving? What specific areas of research interest you the most?\n",
      "2. How do you think your research interests align with the RL master thesis role?\n",
      "\n",
      "**General Questions**\n",
      "\n",
      "1. Can you tell me about a challenging project you worked on and how you overcame obstacles?\n",
      "2. How do you handle feedback and criticism on your work?\n",
      "3. What are your long-term career goals, and how does this RL master thesis role fit into your plans?\n",
      "\n",
      "Remember to tailor your questions to the specific requirements of the RL master thesis role and your organization's needs.\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"What would I need to ask him while interviewing him for the role?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4609b-3515-447f-8f2a-9ded3a1d2b0f",
   "metadata": {},
   "source": [
    "## Running a Model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1313f707-42dd-4a2c-99e9-ef1cda98d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install llama-index llama-cpp-python huggingface-hub --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cbee818-5a73-4fdf-8f4a-6cba4dd5e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "209b5f02-3464-4ee8-9c61-1141f7aef117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f127d7cc-0551-4fc4-a1b2-cb315e8d88e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =  3204.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "...................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified: kv_size = 512, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    64.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =    81.01 MiB\n",
      "llama_context: graph nodes  = 1094\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =    1202.37 ms\n",
      "llama_perf_context_print: prompt eval time =    1201.59 ms /    26 tokens (   46.21 ms per token,    21.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4998.42 ms /    30 runs   (  166.61 ms per token,     6.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    6211.54 ms /    56 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Mommy, my stomach hurts really bad! And cookie monster is chasing me around the house! Can you make it stop?\"\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "llm = Llama(model_path=model_path)\n",
    "\n",
    "response = llm.create_completion(\n",
    "    prompt=\"Act like a 10 year old telling his mother that his stomach hurts and cookie monster is chasing you.\\n\",\n",
    "    max_tokens=200,         # increase this\n",
    "    temperature=0.7,\n",
    "    top_p=0.8\n",
    "        # optional, control stop tokens carefully\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79acbf88-acb9-4d99-b0ae-1e3a1af35364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =  3204.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "...................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   296.01 MiB\n",
      "llama_context: graph nodes  = 1094\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =   36067.55 ms\n",
      "llama_perf_context_print: prompt eval time =   36066.89 ms /  1414 tokens (   25.51 ms per token,    39.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49335.49 ms /   276 runs   (  178.75 ms per token,     5.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   85577.76 ms /  1690 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model answer based on PDF:\n",
      " Rochan Kumar Saravanakannan is an Indian national born on June 30th, 2001. He has a Master's degree in Autonomous Driving from Hochschule Coburg and a Bachelor's degree in Computer Science Engineering from Nitte Meenakshi Institute of Technology. Rochan has experience working as a Working Student at Valeo and an Intern at Vodafone Intelligent Solutions. He has expertise in Python, ROS2, Git, Computer Vision, Deep Learning, and Sensor Fusion. Rochan has worked on projects such as Reinforcement Learning for Autonomous Racing, Number Plate Recognition, Traffic Vehicle Counting and Tracking, and Lane Detection System. He has also completed a Bachelor's thesis titled \"Multipurpose Application for a Smart Campus\". Rochan holds several certifications, including Fundamentals of Linux and Data Analytics and Artificial Intelligence and Machine Learning Basics. He has proficiency in English, German, and Tamil. Rochan has achieved success in coding hackathons, has ranked in the top 40 teams in India during an internship, and has won second place in a theatre performance. He has also volunteered with NGOs.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from llama_cpp import Llama\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    return full_text\n",
    "\n",
    "def query_llama_with_pdf(pdf_path, model_path, user_question):\n",
    "    # Extract text from PDF\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Construct prompt: you can customize this as you want\n",
    "    prompt = (\n",
    "        \"You are an assistant that answers questions based on the following document:\\n\\n\"\n",
    "        + pdf_text\n",
    "        + \"\\n\\nQuestion: \" + user_question + \"\\nAnswer:\"\n",
    "    )\n",
    "    \n",
    "    # Load the model\n",
    "    llm = Llama(model_path=model_path, n_ctx=4096)\n",
    "    \n",
    "    # Generate response\n",
    "    response = llm.create_completion(\n",
    "        prompt=prompt,\n",
    "        max_tokens=300,    # increase max tokens for longer answers\n",
    "        temperature=0.7,\n",
    "        top_p=0.8,\n",
    "        # add stop tokens if needed, e.g., stop=[\"\\n\\n\"]\n",
    "    )\n",
    "    \n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "# Example usage\n",
    "pdf_file = \"CV.pdf\"\n",
    "model_file = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "question = \"Summarize the main points of the document.\"\n",
    "\n",
    "answer = query_llama_with_pdf(pdf_file, model_file, question)\n",
    "print(\"Model answer based on PDF:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f7bf5a4-549f-4208-acb5-68296cdfa69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =  3204.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "...................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   296.01 MiB\n",
      "llama_context: graph nodes  = 1094\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =   40322.18 ms\n",
      "llama_perf_context_print: prompt eval time =   40321.19 ms /  1435 tokens (   28.10 ms per token,    35.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28601.88 ms /   143 runs   (  200.01 ms per token,     5.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   68998.09 ms /  1578 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model answer based on PDF:\n",
      " Based on the provided information, it appears that Rochan Kumar Saravanakannan has a strong background in computer science and engineering, with experience in software development and research in autonomous driving and reinforcement learning. He has demonstrated skills in Python, Linux, Git, and various machine learning libraries. He has also worked on projects related to number plate recognition, traffic vehicle counting and tracking, and lane detection. Given his experience and skills, he could be a good fit for a basic German-speaking research institute with a focus on RL for trajectory and traffic-related applications. However, it would be important to further assess his fit based on specific requirements and expectations of the institute.\n"
     ]
    }
   ],
   "source": [
    "question = \"Is he a good fit for a basic german, advanced english speaking research institute with RL focus for trajectory and traffic related applciations\"\n",
    "\n",
    "answer = query_llama_with_pdf(pdf_file, model_file, question)\n",
    "print(\"Model answer based on PDF:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634d8ff-9ffb-4392-a3ca-83f57735a94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310_env)",
   "language": "python",
   "name": "py310_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
